{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本相似度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/MuJiang0618/NLP/blob/master/%E5%A6%82%E4%BD%95%E8%AE%A1%E7%AE%97%E4%B8%A4%E4%B8%AA%E6%96%87%E6%A1%A3%E7%9A%84%E7%9B%B8%E4%BC%BC%E5%BA%A6.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\study\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format=' %(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Shipment of gold damaged in a fire\",\n",
    "            \"Delivery of silver arrived in a silver truck\",\n",
    "             \"Shipment of gold arrived in a truck\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "全部转化为小写"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['shipment', 'of', 'gold', 'damaged', 'in', 'a', 'fire'], ['delivery', 'of', 'silver', 'arrived', 'in', 'a', 'silver', 'truck'], ['shipment', 'of', 'gold', 'arrived', 'in', 'a', 'truck']]\n"
     ]
    }
   ],
   "source": [
    "texts = [[word for word in document.lower().split()] for document in documents]\n",
    "print(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-10-08 21:35:41,060 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      " 2018-10-08 21:35:41,060 : INFO : built Dictionary(11 unique tokens: ['a', 'damaged', 'fire', 'gold', 'in']...) from 3 documents (total 22 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 0, 'damaged': 1, 'fire': 2, 'gold': 3, 'in': 4, 'of': 5, 'shipment': 6, 'arrived': 7, 'delivery': 8, 'silver': 9, 'truck': 10}\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (4, 1), (5, 1), (7, 1), (8, 1), (9, 2), (10, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (10, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-10-08 22:08:06,121 : INFO : collecting document frequencies\n",
      " 2018-10-08 22:08:06,123 : INFO : PROGRESS: processing document #0\n",
      " 2018-10-08 22:08:06,125 : INFO : calculating IDF weights for 3 documents and 10 features (21 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.6633689723434505), (2, 0.6633689723434505), (3, 0.2448297500958463), (6, 0.2448297500958463)]\n",
      "[(7, 0.16073253746956623), (8, 0.4355066251613605), (9, 0.871013250322721), (10, 0.16073253746956623)]\n",
      "[(3, 0.5), (6, 0.5), (7, 0.5), (10, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "corpus_tfidf = tfidf[corpus]   # 计算每个文档中的词的tf-idf值, 如果corpus含有多个文档, 则返回多个文档中每个词的tf-idf\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 3, 1: 1, 2: 1, 3: 2, 4: 3, 5: 3, 6: 2, 7: 2, 8: 1, 9: 1, 10: 2}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.dfs)   # 0:3 表示包含单词0的文档数为3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.0, 1: 1.5849625007211563, 2: 1.5849625007211563, 3: 0.5849625007211562, 4: 0.0, 5: 0.0, 6: 0.5849625007211562, 7: 0.5849625007211562, 8: 1.5849625007211563, 9: 1.5849625007211563, 10: 0.5849625007211562}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.idfs)   # 每个词的idf值, 注意不是tf-idf值"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "包含 id 为 0，4，5 这3 个单词的文档数(df)都为3，而文档总数也是3，所以idf被计算为0了, 看来 gensim 没有对分子+1，做一个平滑处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-10-08 22:24:22,607 : INFO : using serial LSI version on this node\n",
      " 2018-10-08 22:24:22,609 : INFO : updating model with new documents\n",
      " 2018-10-08 22:24:22,611 : INFO : preparing a new chunk of documents\n",
      " 2018-10-08 22:24:22,612 : INFO : using 100 extra samples and 2 power iterations\n",
      " 2018-10-08 22:24:22,613 : INFO : 1st phase: constructing (11, 102) action matrix\n",
      " 2018-10-08 22:24:22,619 : INFO : orthonormalizing (11, 102) action matrix\n",
      " 2018-10-08 22:24:22,641 : INFO : 2nd phase: running dense svd on (11, 3) matrix\n",
      " 2018-10-08 22:24:22,652 : INFO : computing the final decomposition\n",
      " 2018-10-08 22:24:22,653 : INFO : keeping 2 factors (discarding 23.571% of energy spectrum)\n",
      " 2018-10-08 22:24:22,655 : INFO : processed documents up to #3\n",
      " 2018-10-08 22:24:22,657 : INFO : topic #0(1.137): -0.438*\"gold\" + -0.438*\"shipment\" + -0.366*\"arrived\" + -0.366*\"truck\" + -0.345*\"fire\" + -0.345*\"damaged\" + -0.297*\"silver\" + -0.149*\"delivery\" + 0.000*\"a\" + 0.000*\"in\"\n",
      " 2018-10-08 22:24:22,658 : INFO : topic #1(1.000): -0.728*\"silver\" + -0.364*\"delivery\" + 0.364*\"fire\" + 0.364*\"damaged\" + -0.134*\"arrived\" + -0.134*\"truck\" + 0.134*\"gold\" + 0.134*\"shipment\" + 0.000*\"a\" + -0.000*\"in\"\n",
      " 2018-10-08 22:24:22,659 : INFO : topic #0(1.137): -0.438*\"gold\" + -0.438*\"shipment\" + -0.366*\"arrived\" + -0.366*\"truck\" + -0.345*\"fire\" + -0.345*\"damaged\" + -0.297*\"silver\" + -0.149*\"delivery\" + 0.000*\"a\" + 0.000*\"in\"\n",
      " 2018-10-08 22:24:22,660 : INFO : topic #1(1.000): -0.728*\"silver\" + -0.364*\"delivery\" + 0.364*\"fire\" + 0.364*\"damaged\" + -0.134*\"arrived\" + -0.134*\"truck\" + 0.134*\"gold\" + 0.134*\"shipment\" + 0.000*\"a\" + -0.000*\"in\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-0.438*\"gold\" + -0.438*\"shipment\" + -0.366*\"arrived\" + -0.366*\"truck\" + -0.345*\"fire\" + -0.345*\"damaged\" + -0.297*\"silver\" + -0.149*\"delivery\" + 0.000*\"a\" + 0.000*\"in\"'),\n",
       " (1,\n",
       "  '-0.728*\"silver\" + -0.364*\"delivery\" + 0.364*\"fire\" + 0.364*\"damaged\" + -0.134*\"arrived\" + -0.134*\"truck\" + 0.134*\"gold\" + 0.134*\"shipment\" + 0.000*\"a\" + -0.000*\"in\"')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)\n",
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, -0.6721146880987869), (1, 0.5488068211935582)]\n",
      "[(0, -0.4412482520869763), (1, -0.8359492048033912)]\n",
      "[(0, -0.804013789637927)]\n"
     ]
    }
   ],
   "source": [
    "corpus_lsi = lsi[corpus_tfidf]\n",
    "for doc in corpus_lsi:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看出, 文档1, 2和topic2相关, 文档3和topic1负相关"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面跑一个LDA看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 2018-10-08 22:33:14,158 : INFO : using symmetric alpha at 0.5\n",
      " 2018-10-08 22:33:14,160 : INFO : using symmetric eta at 0.5\n",
      " 2018-10-08 22:33:14,161 : INFO : using serial LDA version on this node\n",
      " 2018-10-08 22:33:14,168 : INFO : running online (single-pass) LDA training, 2 topics, 1 passes over the supplied corpus of 3 documents, updating model once every 3 documents, evaluating perplexity every 3 documents, iterating 50x with a convergence threshold of 0.001000\n",
      " 2018-10-08 22:33:14,169 : WARNING : too few updates, training might not converge; consider increasing the number of passes or iterations to improve accuracy\n",
      " 2018-10-08 22:33:14,176 : INFO : -4.106 per-word bound, 17.2 perplexity estimate based on a held-out corpus of 3 documents with 5 words\n",
      " 2018-10-08 22:33:14,177 : INFO : PROGRESS: pass 0, at document #3/3\n",
      " 2018-10-08 22:33:14,179 : INFO : topic #0 (0.500): 0.117*\"damaged\" + 0.115*\"silver\" + 0.112*\"fire\" + 0.096*\"delivery\" + 0.094*\"truck\" + 0.093*\"gold\" + 0.090*\"arrived\" + 0.089*\"shipment\" + 0.065*\"a\" + 0.065*\"in\"\n",
      " 2018-10-08 22:33:14,180 : INFO : topic #1 (0.500): 0.121*\"shipment\" + 0.118*\"gold\" + 0.113*\"silver\" + 0.111*\"arrived\" + 0.108*\"truck\" + 0.091*\"fire\" + 0.087*\"damaged\" + 0.079*\"delivery\" + 0.057*\"a\" + 0.057*\"in\"\n",
      " 2018-10-08 22:33:14,180 : INFO : topic diff=0.410177, rho=1.000000\n",
      " 2018-10-08 22:33:14,181 : INFO : topic #0 (0.500): 0.117*\"damaged\" + 0.115*\"silver\" + 0.112*\"fire\" + 0.096*\"delivery\" + 0.094*\"truck\" + 0.093*\"gold\" + 0.090*\"arrived\" + 0.089*\"shipment\" + 0.065*\"a\" + 0.065*\"in\"\n",
      " 2018-10-08 22:33:14,181 : INFO : topic #1 (0.500): 0.121*\"shipment\" + 0.118*\"gold\" + 0.113*\"silver\" + 0.111*\"arrived\" + 0.108*\"truck\" + 0.091*\"fire\" + 0.087*\"damaged\" + 0.079*\"delivery\" + 0.057*\"a\" + 0.057*\"in\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.117*\"damaged\" + 0.115*\"silver\" + 0.112*\"fire\" + 0.096*\"delivery\" + 0.094*\"truck\" + 0.093*\"gold\" + 0.090*\"arrived\" + 0.089*\"shipment\" + 0.065*\"a\" + 0.065*\"in\"'),\n",
       " (1,\n",
       "  '0.121*\"shipment\" + 0.118*\"gold\" + 0.113*\"silver\" + 0.111*\"arrived\" + 0.108*\"truck\" + 0.091*\"fire\" + 0.087*\"damaged\" + 0.079*\"delivery\" + 0.057*\"a\" + 0.057*\"in\"')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda = models.LdaModel(corpus_tfidf, id2word=dictionary,num_topics=2)\n",
    "lda.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个主题中, 权值越大的词, 越与该主题相关; 两个主题中对应词的权重值差不多, 没有说服力"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
