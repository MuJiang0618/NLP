{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 复旦新闻物料库文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理\n",
    "引入语料库, 去除特殊符号, 并进行分词处理\n",
    "选择3个分类, 每个分类15篇文档"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 载入语料库\n",
    "这里的语料库格式不太统一, 所以加载不方便"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\levovo\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.458 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "r1 =  '[a-zA-Z0-9’\\n·\\s＊!\"：#$%&\\'()◆●（）＠②*+,-./:;<=>?@，。?★、…【】《》？——“”‘’！[\\$$^_`{|}~]+'\n",
    "\n",
    "dic = {4: 'C4-Literature', 5: 'C5-Education', 6: 'C6-Philosophy', 17: 'C17-Communication'}\n",
    "\n",
    "y_train = []; y_test = []\n",
    "corpus_train = [] ; corpus_test = []\n",
    "num_classes = 3\n",
    "num_doc = 32   # 最后一篇文档的编号\n",
    "# num_testdoc = 3\n",
    "       \n",
    "# D:\\CS\\dataset\\NLP\\answer\\C3-Art\\C3-Art0002.txt\n",
    "counter_testSet = 0\n",
    "for i in range(10, num_doc+2, 2):\n",
    "    with open('D:\\\\CS\\\\dataset\\\\NLP\\\\answer\\\\' + dic[4] + '\\\\' + dic[4] + str(i) + '.txt', 'r', encoding='utf-8') as f:\n",
    "        counter_testSet += 1 \n",
    "        text = f.read()\n",
    "        text = re.sub(r1, '', text)\n",
    "        corpus = ' '.join(jieba.cut(text))\n",
    "        if counter_testSet < 10:\n",
    "            y_train.append(4)\n",
    "            corpus_train.append(corpus)\n",
    "        else:\n",
    "            y_test.append(4)\n",
    "            corpus_test.append(corpus)\n",
    "            \n",
    "counter_testSet = 0\n",
    "for i in range(10, num_doc+2, 2):\n",
    "    with open('D:\\\\CS\\\\dataset\\\\NLP\\\\answer\\\\' + dic[5] + '\\\\' + dic[5] + '0'+ str(i) + '.txt', 'r', encoding='utf-8') as f:\n",
    "        counter_testSet += 1\n",
    "        text = f.read()\n",
    "        text = re.sub(r1, '', text)\n",
    "        corpus = ' '.join(jieba.cut(text))\n",
    "        if counter_testSet < 10:\n",
    "            y_train.append(5)\n",
    "            corpus_train.append(corpus)\n",
    "        else:\n",
    "            y_test.append(5)\n",
    "            corpus_test.append(corpus)\n",
    "            \n",
    "counter_testSet = 0\n",
    "for i in range(10, num_doc+2, 2):\n",
    "    with open('D:\\\\CS\\\\dataset\\\\NLP\\\\answer\\\\' + dic[6] + '\\\\' + dic[6] + str(i) + '.txt', 'r', encoding='utf-8') as f:\n",
    "        counter_testSet += 1\n",
    "        text = f.read()\n",
    "        text = re.sub(r1, '', text)\n",
    "        corpus = ' '.join(jieba.cut(text))\n",
    "        if counter_testSet < 9:\n",
    "            y_train.append(6)\n",
    "            corpus_train.append(corpus)\n",
    "        else:\n",
    "            y_test.append(6)\n",
    "            corpus_test.append(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'日期 版号 标题 为雅 文化 升温 添柴 东方出版社 重版 一批 现代 学术 名著 作者 祝华新 正文 本报记者 祝华新 新年伊始 郭沫若 十 批判 书 等 一批 民国时期 的 著名 学术著作 带 着 一缕 淡淡的 墨香以 简体 横排 的 新 版本 走 下 印刷机 这 就是 东方出版社 献给 莘莘学子 的 新年礼物 民国 学术 经典 文库 解放后 一些 出版社 曾经 重印 或 影印 过 民国时期 部分 学术著作 但 较为 零碎 未 成 系统 而且 旧版 的 繁体 竖排 给 当代 中青年 读者 带来 诸多不便 该 文库 从 民国时期 总书 目的 十余万 种 中文 图书 中 精选 出 ３ １ 种 著作 展示 了 民国时期 在 思想史 历史 文学史 等 方面 的 重要 学术 成果 学术 再 上层 楼 的 阶梯 东西 文化 大规模 碰撞 导致 的 中国 传统 学术 向 现代 学术 的 演变 自 晚清 开始 辛亥革命 后 才 取得 长足 的 进展 二三十 年代 的 学界 可谓 杂花生树 群莺 乱飞 百舸争流 正如 著名 学者 张岱年 在 总序 中所言 当时 许多 学者 继承 了 清代 朴学 的 作风 考据 比较 精审 ； 也 有些 学者 对于 宋明理学 有 较 多 的 了解 对于 深邃 的 义理 有 较 深 的 体会 ； 同时 许多 学者 都 在 一定 程度 上 参考 了 西方 的 治学 方法 致力于 中西 学术 的 会通 与 融合 因而 达到 了 学术研究 的 较 高水平 收入 文库 的 作者 中 有些 是 今人 非常 熟悉 的 如 梁启超 蔡元培 鲁迅 陈寅恪 陶行知 阿英 郑振铎 等 但 熟悉 中 又 有点 陌生 比如 很多 人 知道 蔡和森 的 名字 但 未必 了解 这位 中共 早期 理论家 还 著 有 一本 社会 进化史 在 严复 鼓吹 的 达尔文主义 基础 上 初步 引入 马克思主义 的 历史观 而 有些 作者 连 相关 专业 的 研究生 都 没听说过 有些 著作 即使 在 北京 图书馆 也 查不到 比如 晚明 思想 史论 的 作者 嵇文甫 解放后 曾 出任 河南省 副省长 后来 又 主动 辞职 到 郑州大学 当 一名 教书匠 可叹 这 段 学苑 佳话 早已 湮没 在 岁月 的 深处 这 批学 人大 都 有 国学 的 扎实 根基 又 周游列国 饱经 欧风美雨 的 栉沐 学贯中西 学 通古今 而且 有 不少 像 梁启超 这样 的 人 由 学术 而 从政 最后 再 返回 学界 对 国情 自有 一番 独到 的 体察 文库 主编 东方出版社 社长 薛德震 断言 民国 学术 尽管 有 历史 的 局限性 甚至 谬误 但 毕竟 是 一个 时代 的 产物 是 当代 学 人 登上 新高峰 的 阶梯 出版 的 一个 重要 功能 是 文化 积累 这 套书 的 目的 就是 为 今人 推荐 一些 有 学术 价值 和 可读性 的 历史 遗产 文库 的 选题 别出心裁 独具慧眼 文学史 类丛 没有 收入 那种 从 先秦散文 一直 写 到 明清小说 的 通史 类 著作 而是 按 文学 体裁 来 划分 选中 乐府 文学史 词曲 史诗 史 中国 散文 史 中国 纯文学 史纲 中国 俗 文学史 等 专著 思想史 类丛 摒弃 了 文革时期 以 唯物 唯心 之争 甚至 是 儒法 斗争 为主 线 的 简单化 做法 而 选择 断代史 如 先秦 政治 思想史 汉代 学术史 略 魏晋 的 自然主义 理学 纲要 两宋 思想 述评 晚清 思想 史论 等 历史 类丛 既 不是 通史 断代史 也 不是 纪传史 而是 令人 耳目一新 的 领域 史如 中国 民族史 中国 风俗 史 中国 伦理学史 中国 宗教 思想史 大纲 中国 教育 改造 等 这种 编选 方法 旨在 为 重新 审视 历史 提供 一个 新 的 视点 使 国人 对 中国 传统 文化 的 认知 从 某些 陈说 的 皮相之见 深入 到 实证 的 科学 的 研究 之中 为 合法 版权 而 奔波 文库 的 作者 已 全部 谢世 但 文库 的 编辑 们 并 没有 因此 而 忽略 新版 的 合法 授权 中国 骈文 史 作者 卢前 下落不明 他们 从 现在 中国社科院 工作 的 另 一位 民国 学人 吴梅 的 弟子 那里 得知 卢前 ６ ０ 年代 逝世 于 南京 就 往 南京 打 了 三四十个 长途电话 南京大学中文系 主任 介绍 说 有人 写过 卢前 传 不巧 的 是 作者 到 苏州 改稿 去 了 好不容易 在 苏州 找到 传记 作者 这才 了解 到 卢前 身后 的 情况 卢前 有 ５ 个 子女 但 他们 手头 都 没有 一点 父亲 的 著作 资料 最后 是 大 女儿 作主 授权 出版 国学 研究 毋忘 五四 传统 曾经 有人 鉴于 每次 考古 大 发现 都 在 某种程度 上 改写 了 中国 的 某 一段 历史 而 发出 这样 的 惊叹 中国 地下 究竟 埋藏 着 多少 秘密 文库 副主编 姜渭渔 的 心头 则 别 有 一番 感受 中国 地上 的 宝贝 更 多 需要 我们 发掘 整理 我们 在 世纪末 遇到 的 很多 问题 早 在 本世纪初 就 已经 有人 作 过 探究 ８ ０ 年代 的 文化 热中 不少 热门话题 其实 是 炒 五四 时期 的 冷饭 而且 某些 议论 未必 有 五四 大师 精当 雅 文化 的 回升 成为 ９ ０ 年代 中期 引人注目 的 现象 新一轮 文化 热对 ８ ０ 年代 的 学风 空疏 作出 深刻 反省 注重 对 文化 典籍 的 研读 而 不是 泛泛 的 中西文化 之论 这 是 可喜 的 进步 但 我们 毕竟 不能 用 中古 乃至 远古 时期 的 国学 来 取代 现代 文化 不能 无视 辛亥革命 尤其 是 五四新文化运动 以来 形成 的 新 的 文化 传统 不能 排斥 中国 文化 母体 中 包括 马克思主义 在内 的 新 的 文明 因子 从 这个 意义 上 来说 这部 总 投资 近 ２ ０ ０ 万元 的 文库 是 东方出版社 继 成功 推出 东方 书林 之旅 之后 又 一 引导 文化 消费 的 得意之作 在 年前 的 郑州 订货会 上 他们 只 凭 一纸 书目 简介 就 赢得 １ ０ ０ ０ 套 的 订数 深圳 一家 个体 书店 的 老板 张口 就要 ５ ０ ０ 套 贵州 两位 大学生 开办 的 书店 也 各要 １ ０ ０ 套 更 有 京城 十几家 大学 书店 闻风而动 要求 进货 薛德震 满怀信心 地说 文库 第一版 的 印数 都 近 ８ ０ ０ ０ 册 我们 有着 良好 的 市场 基础 这 套书 绝不会 赔本'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_test[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corpus train length:  26\n",
      "corpus test length :  10\n"
     ]
    }
   ],
   "source": [
    "print('corpus train length: ', len(corpus_train))\n",
    "print('corpus test length : ', len(corpus_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入停用词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#从文件导入停用词表\n",
    "stpwrdpath = r\"D:\\Repositories\\Datasets\\NLP\\chineseStopWords.txt\"\n",
    "stpwrd_dic = open(stpwrdpath, 'r', encoding='UTF-8-sig')\n",
    "# stpwrd_dic = open('new2.txt',  'r', encoding='UTF-8-sig'\n",
    "stpwrd_content = stpwrd_dic.read()\n",
    "#将停用词表转换为list  \n",
    "stpwrdlst = stpwrd_content.splitlines()\n",
    "stpwrd_dic.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['啊', '阿', '哎', '一个', '一举', '一体', '一册', '一动', '一出', '一只', '一员', '一大', '一套', '一对', '一派', '一层', '一副', '一座', '一把', '一本', '一束', '一步', '一段', ' 一段时间', ' 一派', ' 一点', ' 一碧', ' 一种', ' 一窥', ' 一章', ' 一类', ' 一系列', ' 一级', ' 一经', ' 一股', ' 一节', ' 一著', ' 一词', ' 一道', ' 一部', ' 一部分', ' 一门', ' 一阵阵', ' 一面镜子', ' 一项', ' 一须', ' 一颗', '三十', '三大', '三层', '三年', '三性', '哎呀', '哎哟', '唉', '俺', '俺们', '按', '按照', '吧', '吧哒', '把', '罢了', '被', '本', '广阔', '本着', '开拓', '拓展', '开拓', '比', '比方', '比如', '鄙人', '彼', '彼此', '边', '别', '别的', '别说', '并', '并且', '三个', '不比', '不成', '不单', '不但', '不独', '不管', '不光', '不过', '不仅', '不拘', '不论', '不怕', '不然', '不如', '不特', '不惟', '不问', '不只', '朝', '朝着', '趁', '趁着', '乘', '冲', '除', '除此之外', '除非', '除了', '此', '此间', '此外', '从', '从而', '出', '打', '待', '但', '但是', '当', '当着', '到', '得', '的', '的话', '等', '等等', '地', '第', '叮咚', '对', '对于', '多', '多少', '而', '而况', '而且', '而是', '而外', '而言', '而已', '尔后', '反过来', '反过来说', '反之', '非但', '非徒', '否则', '嘎', '嘎登', '刚', '刚刚', '该', '赶', '个', '各', '各个', '各位', '各种', '各自', '给', '根据', '跟', '故', '故此', '固然', '关于', '管', '归', '果然', '果真', '过', '哈', '哈哈', '呵', '和', '何', '何处', '何况', '何时', '嘿', '哼', '哼唷', '呼哧', '乎', '哗', '还是', '还有', '换句话说', '换言之', '或', '或是', '或者', '极了', '及', '及其', '及至', '即', '即便', '即或', '即令', '即若', '即使', '几', '几时', '己', '既', '既然', '既是', '继而', '加之', '假如', '假若', '假使', '鉴于', '将', '较', '较之', '叫', '接着', '结果', '借', '紧接着', '进而', '尽', '尽管', '经', '经过', '就', '就是', '就是说', '据', '具体地说', '具体说来', '开始', '开外', '靠', '咳', '可', '可见', '可是', '可以', '况且', '啦', '来', '来着', '离', '例如', '哩', '连', '连同', '两者', '了', '临', '另', '另外', '另一方面', '论', '嘛', '吗', '慢说', '漫说', '冒', '么', '每', '每当', '们', '莫若', '某', '某个', '某些', '拿', '哪', '哪边', '哪儿', '哪个', '哪里', '哪年', '哪怕', '哪天', '哪些', '哪样', '那', '那边', '那儿', '那个', '那会儿', '那里', '那么', '那么些', '那么样', '那时', '那些', '那样', '乃', '乃至', '呢', '能', '你', '你们', '您', '宁', '宁可', '宁肯', '宁愿', '哦', '呕', '啪达', '旁人', '呸', '凭', '凭借', '其', '其次', '其二', '其他', '其它', '其一', '其余', '其中', '却', '去', '起', '起见', '起见', '岂但', '恰恰相反', '前后', '前者', '且', '然而', '然后', '然则', '让', '人家', '任', '任何', '任凭', '如', '如此', '如果', '如何', '如其', '如若', '如上所述', '若', '若非', '若是', '啥', '上下', '尚且', '设若', '设使', '甚而', '甚么', '甚至', '省得', '时候', '十分', '什么', '什么样', '使得', '是', '是的', '首先', '谁', '谁知', '顺', '顺着', '似的', '虽', '虽然', '虽说', '虽则', '随', '随着', '所', '所以', '他', '他们', '他人', '它', '它们', '她', '她们', '倘', '倘或', '倘然', '倘若', '倘使', '腾', '替', '通过', '同', '同时', '哇', '万一', '往', '望', '为', '为何', '为了', '为什么', '为着', '喂', '嗡嗡', '我', '我们', '呜', '呜呼', '乌乎', '无论', '无宁', '毋宁', '嘻', '吓', '相对而言', '像', '向', '向着', '嘘', '呀', '焉', '沿', '沿着', '要', '要不', '要不然', '要不是', '要么', '要是', '也', '也罢', '也好', '一一', '———', '一般', '一边', '一会儿', '一旦', '一定', '一点点', '一方面', '一面', '一来', '一起', '一切', '一下', '一下子', '一样', '一些', '一则', '一直', '依', '依照', '矣', '以', '以便', '以及', '以免', '以至', '以至于', '以致', '抑或', '因', '因此', '因而', '因为', '哟', '用', '由', '由此可见', '由于', '有', '有的', '有关', '有些', '又', '于', '于是', '于是乎', '与', '与此同时', '与否', '与其', '越是', '云云', '哉', '再说', '再者', '在', '在下', '咱', '咱们', '则', '怎', '怎么', '怎么办', '怎么样', '怎样', '咋', '照', '照着', '者', '这', '这边', '这儿', '这个', '这会儿', '这就是说', '这里', '这么', '这么点儿', '这么些', '这么样', '这时', '这些', '这样', '正如', '吱', '之', '之类', '之所以', '之一', '只是', '只限', '只要', '只有', '至', '至于', '诸位', '着', '着呢', '自', '自从', '自个儿', '自各儿', '自己', '自家', '自身', '综上所述', '总的来看', '总的来说', '总的说来', '总而言之', '总之', '纵', '纵令', '纵然', '纵使', '遵照', '作为', '兮', '呃', '呗', '咚', '咦', '喏', '啐', '喔唷', '嗬', '嗯', '嗳', '上', '下', '不', '不再', '不同', '不少', '一', '二', '两', '三', '四', '五', '六', '七', '八', '九', '十', '个人', '中', '之前', '之后', '之外', '之间', '也许', '人', '前', '前天', '今天', '今年', '明天', '明年', '后天', '去年', '从来', '令', '以下', '以前', '以后', '以往', '以来', '份', '件', '伏', '众', '众多', '会', '位', '做', '停', '顶', '先', '先前', '全', '公司', '公斤', '其实', '内', '已', '再', '小', '大', '还', '里', '都', '部', '遍', '道', '说', '第一', '第二', '第三', '第四', '第五', '第六', '第七', '第八', '第九', '第十', '只', '后', '太', '大家', '女', '男', '看', '新浪', '年', '很', '才', '时', '更', '最', '本报', '讯', '记者', '演', '片', '一', '一下', '一些', '一切', '一则', '一天', '一定', '一方面', '一旦', '一时', '一来', '一样', '一次', '一片', '一直', '一致', '一般', '一起', '一边', '一面', '万一', '上下', '上升', '上去', '上来', '上述', '上面', '下列', '下去', '一般', '下来', '下面', '不一', '不久', '不仅', '不会', '不但', '不光', '不单', '不变', '不只', '不可', '不同', '不够', '不如', '不得', '不怕', '不惟', '不成', '不拘', '不敢', '不断', '不是', '不比', '不然', '不特', '不独', '不管', '不能', '不要', '不论', '不足', '不过', '不问', '与', '与其', '与否', '与此同时', '专门', '且', '两者', '严格', '严重', '个', '个人', '个别', '中小', '中间', '丰富', '临', '为', '为主', '为了', '为什么', '为什麽', '为何', '为着', '主张', '主要', '举行', '乃', '乃至', '么', '之', '之一', '之前', '之后', '之後', '之所以', '之类', '乌乎', '乎', '乘', '也', '也好', '也是', '也罢', '了', '了解', '争取', '于', '于是', '于是乎', '云云', '互相', '产生', '人们', '人家', '什么', '什么样', '什麽', '今后', '今天', '今年', '今後', '仍然', '从', '从事', '从而', '他', '他人', '他们', '他的', '代替', '以', '以上', '以下', '以为', '以便', '以免', '以前', '以及', '以后', '以外', '以後', '以来', '以至', '以至于', '以致', '们', '任', '任何', '任凭', '任务', '企图', '伟大', '似乎', '似的', '但', '但是', '何', '何况', '何处', '何时', '作为', '你', '你们', '你的', '使得', '使用', '例如', '依', '依照', '依靠', '促进', '保持', '俺', '俺们', '倘', '倘使', '倘或', '倘然', '倘若', '假使', '假如', '假若', '做到', '像', '允许', '充分', '先后', '先後', '先生', '全部', '全面', '兮', '共同', '关于', '其', '其一', '其中', '其二', '其他', '其余', '其它', '其实', '其次', '具体', '具体地说', '具体说来', '具有', '再者', '再说', '冒', '冲', '决定', '况且', '准备', '几', '几乎', '几时', '凭', '凭借', '出去', '出来', '出现', '分别', '则', '别', '别的', '别说', '到', '前后', '前者', '前进', '前面', '加之', '加以', '加入', '加强', '十分', '即', '即令', '即使', '即便', '即或', '即若', '却不', '原来', '又', '及', '及其', '及时', '及至', '双方', '反之', '反应', '反映', '反过来', '反过来说', '取得', '受到', '变成', '另', '另一方面', '另外', '只是', '只有', '只要', '只限', '叫', '叫做', '召开', '叮咚', '可', '可以', '可是', '可能', '可见', '各', '各个', '各人', '各位', '各地', '各种', '各级', '各自', '合理', '同', '同一', '同时', '同样', '后来', '后面', '向', '向着', '吓', '吗', '否则', '吧', '吧哒', '吱', '呀', '呃', '呕', '呗', '呜', '呜呼', '呢', '周围', '呵', '呸', '呼哧', '咋', '和', '咚', '咦', '咱', '咱们', '咳', '哇', '哈', '哈哈', '哉', '哎', '哎呀', '哎哟', '哗', '哟', '哦', '哩', '哪', '哪个', '哪些', '哪儿', '哪天', '哪年', '哪怕', '哪样', '哪边', '哪里', '哼', '哼唷', '唉', '啊', '啐', '啥', '啦', '啪达', '喂', '喏', '喔唷', '嗡嗡', '嗬', '嗯', '嗳', '嘎', '嘎登', '嘘', '嘛', '嘻', '嘿', '因', '因为', '因此', '因而', '固然', '在', '在下', '地', '坚决', '坚持', '基本', '处理', '复杂', '多', '多少', '多数', '多次', '大力', '大多数', '大大', '大家', '大批', '大约', '大量', '失去', '她', '她们', '她的', '好的', '好象', '如', '如上所述', '如下', '如何', '如其', '如果', '如此', '如若', '存在', '宁', '宁可', '宁愿', '宁肯', '它', '它们', '它们的', '它的', '安全', '完全', '完成', '实现', '实际', '宣布', '容易', '密切', '对', '对于', '对应', '将', '少数', '尔后', '尚且', '尤其', '就', '就是', '就是说', '尽', '尽管', '属于', '岂但', '左右', '巨大', '巩固', '己', '已经', '帮助', '常常', '并', '并不', '并不是', '并且', '并没有', '广大', '广泛', '应当', '应用', '应该', '开外', '开始', '开展', '引起', '强烈', '强调', '归', '当', '当前', '当时', '当然', '当着', '形成', '彻底', '彼', '彼此', '往', '往往', '待', '後来', '後面', '得', '得出', '得到', '心里', '必然', '必要', '必须', '怎', '怎么', '怎么办', '怎么样', '怎样', '怎麽', '总之', '总是', '总的来看', '总的来说', '总的说来', '总结', '总而言之', '恰恰相反', '您', '意思', '愿意', '慢说', '成为', '我', '我们', '我的', '或', '或是', '或者', '战斗', '所', '所以', '所有', '所谓', '打', '扩大', '便', '把', '抑或', '拿', '按', '按照', '换句话说', '换言之', '据', '掌握', '接着', '接著', '故', '故此', '整个', '方便', '方面', '旁人', '无宁', '无法', '无论', '整体', '整', '既', '既是', '既然', '时候', '明显', '明确', '是', '是否', '是的', '显然', '显著', '普通', '遍', '普遍', '更加', '曾经', '替', '最后', '最大', '最好', '最後', '最近', '最高', '有', '有些', '有关', '有利', '有力', '有所', '有效', '有时', '有点', '有的', '有着', '有著', '望', '朝', '朝着', '本', '本着', '来', '来着', '极了', '构成', '果然', '果真', '某', '某个', '某些', '根据', '根本', '欢迎', '正在', '正如', '正常', '此', '此外', '此时', '此间', '毋宁', '每', '每个', '每天', '每年', '每当', '比', '比如', '比方', '比较', '毫不', '没有', '沿', '沿着', '注意', '深入', '清楚', '满足', '漫说', '焉', '然则', '然后', '然後', '然而', '照', '照着', '特别是', '特殊', '特点', '现代', '现在', '甚么', '甚而', '甚至', '用', '由', '由于', '由此可见', '的', '的话', '目前', '直到', '直接', '相似', '相信', '相反', '相同', '相对', '相对而言', '相应', '相当', '相等', '省得', '看出', '看到', '看来', '看看', '看见', '真是', '真正', '着', '着呢', '矣', '知道', '确定', '离', '积极', '移动', '突出', '突然', '立即', '第', '等', '等等', '管', '紧接着', '纵', '纵令', '纵使', '纵然', '练习', '组成', '经', '经常', '经过', '结合', '结果', '给', '绝对', '继续', '继而', '维持', '综上所述', '罢了', '考虑', '者', '而', '而且', '而况', '而外', '而已', '而是', '而言', '联系', '能', '能否', '能够', '腾', '自', '自个儿', '自从', '自各儿', '自家', '自己', '自身', '至', '至于', '倍', '几本', '在', '良好', '若', '若是', '若非', '范围', '莫若', '获得', '虽', '虽则', '虽然', '虽说', '行为', '行动', '表明', '表示', '被', '要', '要么', '要不', '要不是', '要不然', '要么', '要是', '要求', '规定', '觉得', '认为', '认真', '认识', '让', '许多', '多', '论', '设使', '设若', '该', '说明', '诸位', '谁', '谁知', '赶', '起', '起来', '起见', '趁', '趁着', '越是', '跟', '转动', '转变', '转贴', '较', '较之', '边', '达到', '迅速', '过', '过去', '过来', '运用', '还是', '还有', '这', '这个', '这么', '这么些', '这么样', '这么点儿', '这些', '这会儿', '这儿', '这就是说', '这时', '这样', '这点', '这种', '这边', '这里', '这麽', '进入', '进步', '进而', '进行', '连', '连同', '适应', '适当', '适用', '逐步', '逐渐', '通常', '通过', '造成', '遇到', '遭到', '避免', '那', '那个', '那么', '那么些', '那么样', '那些', '那会儿', '那儿', '那时', '那样', '那边', '那里', '那麽', '部分', '鄙人', '采取', '里面', '重大', '重新', '重要', '鉴于', '而', '问题', '防止', '阿', '附近', '限制', '除', '有些', '除了', '除此之外', '除非', '过', '非', '随', '随着', '随著', '集中', '需要', '非但', '非常', '非徒', '了', '多', '了解', '能够', '不顾', '不说', '不管', '不论', '靠', '是', '顺', '他', '她', '它', '说', '讲', '你', '由于', '因为', '顺着', '首先', '高兴', '不', '是不是', '说说', '大法']\n"
     ]
    }
   ],
   "source": [
    "print(stpwrdlst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\study\\Anaconda\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:286: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['一段时间', '一点', '一碧', '一种', '一窥', '一章', '一类', '一系列', '一级', '一经', '一股', '一节', '一著', '一词', '一道', '一部', '一部分', '一门', '一阵阵', '一面镜子', '一项', '一须', '一颗'] not in stop_words.\n",
      "  sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(stop_words=stpwrdlst)\n",
    "merged_corpus = corpus_train.copy()\n",
    "merged_corpus.extend(corpus_test)   # 注意extend方法返回值为None\n",
    "\n",
    "re = tfidf.fit_transform(merged_corpus)     # 这里要同时考虑训练集和测试集语料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3481)\t0.009070169663414574\n",
      "  (0, 4213)\t0.009070169663414574\n",
      "  (0, 3768)\t0.00791438798991609\n",
      "  (0, 4855)\t0.017307278646298857\n",
      "  (0, 1479)\t0.03996306907421868\n",
      "  (0, 685)\t0.01998153453710934\n",
      "  (0, 712)\t0.11995889523250142\n",
      "  (0, 3336)\t0.1879108088676009\n",
      "  (0, 329)\t0.07669968204547602\n",
      "  (0, 2240)\t0.7637350550307946\n",
      "  (0, 255)\t0.045008631574022545\n",
      "  (0, 3364)\t0.049778003179174836\n",
      "  (0, 1828)\t0.036194488736871915\n",
      "  (0, 2199)\t0.03796070182413454\n",
      "  (0, 4505)\t0.06751294736103382\n",
      "  (0, 4862)\t0.05266870670264048\n",
      "  (0, 782)\t0.009324354673651458\n",
      "  (0, 6084)\t0.029374425193492103\n",
      "  (0, 3843)\t0.009070169663414574\n",
      "  (0, 325)\t0.029374425193492103\n",
      "  (0, 4495)\t0.022504315787011273\n",
      "  (0, 2202)\t0.029374425193492103\n",
      "  (0, 3969)\t0.02633435335132024\n",
      "  (0, 4151)\t0.029374425193492103\n",
      "  (0, 3725)\t0.016592667726391612\n",
      "  :\t:\n",
      "  (35, 2695)\t0.02609801492666638\n",
      "  (35, 280)\t0.02609801492666638\n",
      "  (35, 4934)\t0.02609801492666638\n",
      "  (35, 1215)\t0.02609801492666638\n",
      "  (35, 5821)\t0.02609801492666638\n",
      "  (35, 4291)\t0.02609801492666638\n",
      "  (35, 2786)\t0.02609801492666638\n",
      "  (35, 3087)\t0.02609801492666638\n",
      "  (35, 194)\t0.02609801492666638\n",
      "  (35, 1382)\t0.02609801492666638\n",
      "  (35, 1053)\t0.02609801492666638\n",
      "  (35, 2169)\t0.02609801492666638\n",
      "  (35, 4034)\t0.02609801492666638\n",
      "  (35, 994)\t0.02609801492666638\n",
      "  (35, 2594)\t0.02609801492666638\n",
      "  (35, 5134)\t0.02609801492666638\n",
      "  (35, 5565)\t0.02609801492666638\n",
      "  (35, 1417)\t0.02609801492666638\n",
      "  (35, 5171)\t0.02609801492666638\n",
      "  (35, 5995)\t0.02609801492666638\n",
      "  (35, 3896)\t0.02609801492666638\n",
      "  (35, 3998)\t0.02609801492666638\n",
      "  (35, 1281)\t0.02609801492666638\n",
      "  (35, 4849)\t0.02609801492666638\n",
      "  (35, 5990)\t0.02609801492666638\n"
     ]
    }
   ],
   "source": [
    "print(re) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 然后抽取每篇文档的前N个tfidf, 给每篇文档组成词向量进行分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "(36, 6139)\n"
     ]
    }
   ],
   "source": [
    "idf_mat = re.toarray()\n",
    "print(idf_mat)\n",
    "print(idf_mat.shape)    # 训练集测试集一共36篇文档, 所有文档包含的不重复词个数为6139"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4102, 4108, 2064, 2065, 2072, 4122, 4126, 4133, 46, 2110, 2111, 68, 4185, 90, 4186, 4196, 4197, 102, 2151, 4200, 4205, 4206, 2159, 2158, 2161, 2163, 4223, 129, 4232, 2185, 4233, 2189, 4238, 2191, 2192, 2195, 4244, 2197, 2199, 2206, 2208, 2209, 4257, 4260, 2216, 2224, 2237, 2240, 4289, 4295, 4296, 2252, 4303, 2263, 2264, 222, 4319, 4321, 227, 4326, 230, 2279, 231, 2283, 2286, 4335, 2288, 241, 2290, 4339, 4340, 2294, 2295, 250, 4346, 256, 2306, 2307, 261, 4357, 2313, 2315, 2318, 4367, 4366, 274, 4374, 4390, 4395, 305, 2357, 2358, 4407, 4416, 323, 329, 330, 2379, 4435, 342, 4440, 344, 2395, 349, 2398, 4454, 4472, 4474, 4481, 2434, 387, 4484, 2441, 4494, 2448, 2450, 4498, 406, 4505, 411, 4515, 425, 2479, 4529, 4530, 2483, 2488, 4538, 2491, 4549, 466, 4563, 2516, 2517, 4565, 468, 2525, 4575, 2528, 479, 482, 4579, 486, 2535, 2534, 4583, 4588, 4590, 4591, 4593, 498, 2545, 508, 2556, 2557, 2559, 517, 2566, 524, 531, 2584, 543, 4641, 547, 4643, 559, 560, 2612, 2626, 2627, 582, 2631, 2632, 2630, 587, 589, 593, 2645, 599, 604, 4701, 2653, 610, 4710, 615, 4714, 4715, 4717, 2674, 4726, 2683, 4740, 645, 646, 644, 4744, 4755, 2711, 672, 673, 2723, 2728, 4780, 685, 4787, 698, 2756, 712, 4811, 2763, 2765, 718, 4815, 719, 721, 2768, 4819, 4817, 4822, 746, 4847, 755, 2805, 2806, 759, 4857, 2819, 776, 781, 782, 783, 2833, 2836, 4889, 4897, 4898, 4900, 2856, 4906, 4913, 817, 4915, 2872, 4926, 4927, 831, 833, 838, 847, 2902, 2903, 4950, 2905, 2906, 2909, 2915, 872, 4970, 4971, 2923, 877, 878, 879, 880, 4972, 2934, 2935, 896, 898, 2951, 2954, 906, 911, 2960, 5014, 5016, 2974, 5026, 2981, 934, 2985, 2992, 2994, 952, 5049, 953, 962, 5062, 970, 3018, 5071, 975, 985, 3039, 5090, 3044, 3051, 3053, 5115, 1024, 5131, 5137, 5149, 1058, 1059, 5157, 3112, 3133, 3140, 3142, 5193, 5194, 5196, 3156, 3159, 3163, 5215, 5219, 3174, 1138, 5237, 3190, 5240, 3200, 5253, 5254, 3205, 1167, 5263, 5271, 3226, 3243, 5294, 3247, 3246, 5297, 5300, 1207, 5321, 5326, 3279, 5330, 3287, 1248, 1250, 3300, 3302, 5351, 1256, 3308, 3310, 5361, 5367, 1287, 3336, 3337, 3340, 3343, 3352, 5403, 5405, 3358, 1310, 1312, 3363, 3364, 3367, 5417, 3375, 3377, 5434, 3395, 1353, 5454, 5455, 5463, 3419, 5468, 5479, 3431, 1388, 3437, 5484, 1394, 5491, 1395, 1401, 5497, 5501, 3458, 1411, 1415, 1419, 1432, 1433, 3489, 3500, 1460, 1474, 5570, 1477, 3526, 1479, 5575, 3525, 1487, 5587, 3539, 1495, 5591, 5597, 1508, 5606, 3567, 1521, 1523, 3574, 5634, 3588, 3599, 1559, 5672, 3626, 5675, 1583, 5684, 3637, 5699, 3658, 3659, 1613, 5712, 1621, 3670, 5719, 1624, 5727, 1633, 5729, 3682, 3685, 1640, 3695, 5745, 5746, 3697, 3701, 3707, 3708, 5769, 1675, 3724, 1678, 3731, 3732, 5781, 1694, 1705, 5801, 1707, 5804, 1708, 1709, 3767, 5830, 3809, 3812, 3828, 5877, 1784, 1793, 5891, 3847, 5899, 1808, 1809, 5907, 1812, 1815, 1818, 1828, 3886, 5934, 1840, 5938, 3894, 5946, 5947, 5950, 3903, 3904, 3907, 1866, 5969, 1874, 1875, 5973, 3944, 3945, 1898, 3946, 1900, 5993, 1902, 5999, 1905, 3968, 6018, 1922, 3982, 1935, 3984, 6030, 1944, 6041, 1952, 4003, 4008, 6058, 4020, 6079, 1984, 6092, 4050, 6098, 2007, 6108, 2013, 6110, 4063, 2030, 2032, 6136]\n"
     ]
    }
   ],
   "source": [
    "ID_list = []\n",
    "topN = 20   # 每篇文档提取前topN个tf-idf值对应的词的ID\n",
    "\n",
    "for row in idf_mat: \n",
    "    ID_list.extend(np.argsort(-row)[:topN])\n",
    "\n",
    "ID_list = list(set(ID_list))\n",
    "print(ID_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "也许可以考虑用idf的分位数分别为每篇文档选取topN个词ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features per doc:  533\n"
     ]
    }
   ],
   "source": [
    "print('features per doc: ',len(ID_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面数组每一个元素代表某个词的ID, 经过去重后, topN=30意味着每个文档有794个特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构建文档的词向量矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainSet = []; testSet = []\n",
    "for doc in range(0, len(corpus_train) + len(corpus_test)):\n",
    "    temp_list = []\n",
    "    for ID in ID_list:\n",
    "        temp_list.append(idf_mat[doc][ID])\n",
    "    if doc < len(corpus_train):\n",
    "        trainSet.append(temp_list)\n",
    "    else:\n",
    "        testSet.append(temp_list)\n",
    "\n",
    "trainSet, testSet = np.array(trainSet), np.array(testSet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='saga',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(multi_class='auto', solver='saga')\n",
    "model.fit(trainSet, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           4       1.00      0.67      0.80         3\n",
      "           5       0.60      1.00      0.75         3\n",
      "           6       1.00      0.75      0.86         4\n",
      "\n",
      "   micro avg       0.80      0.80      0.80        10\n",
      "   macro avg       0.87      0.81      0.80        10\n",
      "weighted avg       0.88      0.80      0.81        10\n",
      "\n",
      "[[2 0 0]\n",
      " [1 3 1]\n",
      " [0 0 3]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "y_pred = model.predict(testSet)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cross validation score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9222222222222222"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "tempSet = trainSet.copy()\n",
    "tempSet = np.concatenate((tempSet, testSet), axis=0)\n",
    "templabel = y_train.copy()\n",
    "templabel.extend(y_test)\n",
    "\n",
    "print('cross validation score:')\n",
    "cross_val_score(model, X=tempSet, y=templabel, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import *\n",
    "\n",
    "clf_NB_multinomial = MultinomialNB()\n",
    "cross_val_score(clf_NB_multinomial, tempSet, templabel, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6555555555555557"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_NB_gaussian = GaussianNB()\n",
    "cross_val_score(clf_NB_gaussian, tempSet, templabel, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么 两个NB性能相差这么多? 有必要深究一下"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
