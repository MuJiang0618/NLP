{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\study\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "import collections #用来统计词频\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预处理\n",
    "获取每条评论的最大长度, 以及所有评论中不重复词的个数 \n",
    "\n",
    "建立词到ID的映射, 保存在 word2ID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每条数据的形式如下:\n",
    "\n",
    "1\tThe Da Vinci Code book is just awesome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\study\\Anaconda\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n",
      "D:\\study\\Anaconda\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "r1 = '[’\\n·＊!\"：#$%&\\'()◆●（）＠②*+,-../:;<=>?@，。?★、…【】《》？——“”‘’！[\\$$^_`{|}~]+'\n",
    "r2 = '[\\s]*'\n",
    "\n",
    "label = [] ; text = []\n",
    "max_lenth = 0   # 每个句子的最长长度, 40\n",
    "\n",
    "with open(r'D:\\CS\\dataset\\NLP\\英文情感分类数据集(二分类).txt', 'r+', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.lower()\n",
    "        line = re.sub(r1, '', line)\n",
    "        line = re.split(r2, line)\n",
    "        if (len(line)-1) > max_lenth:\n",
    "            max_lenth = (len(line)-1)\n",
    "            \n",
    "        label.append(int(line[0]))\n",
    "        text.append(line[1:])\n",
    "        \n",
    "from gensim import corpora\n",
    "\n",
    "word2ID = corpora.Dictionary(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_word = len(word2ID.token2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把每个文本中的词用ID表示, 每个文本长度为40, 不足的补零\n",
    "vec_text = []\n",
    "for i in text:\n",
    "    temp_list = []\n",
    "    for word in i:\n",
    "        if word in word2ID.token2id.keys():\n",
    "            temp_list.append(word2ID.token2id[word])\n",
    "        else:\n",
    "            temp_list.append(num_word+1)     # 不存在于字典的词用总词数+1填充\n",
    "            \n",
    "    bu0 = max_lenth - len(temp_list)\n",
    "    temp_list.extend([num_word+2] * bu0)\n",
    "    \n",
    "    vec_text.append(temp_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X, train_y, test_y = train_test_split(np.array(vec_text), label, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 128\n",
    "hidden_layer_size = 64\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(2248+3, embedding_size))   # 将每个batch中的文本的每个词转化为词向量\n",
    "model.add(LSTM(hidden_layer_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation(\"sigmoid\"))\n",
    "model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "↑为什么第5行Embedding层要+3而不是+2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5668 samples, validate on 1418 samples\n",
      "Epoch 1/5\n",
      "5668/5668 [==============================] - 4s 770us/step - loss: 0.6858 - acc: 0.5623 - val_loss: 0.6886 - val_acc: 0.5494\n",
      "Epoch 2/5\n",
      "5668/5668 [==============================] - 3s 552us/step - loss: 0.4754 - acc: 0.7424 - val_loss: 0.1324 - val_acc: 0.9647\n",
      "Epoch 3/5\n",
      "5668/5668 [==============================] - 3s 547us/step - loss: 0.1092 - acc: 0.9739 - val_loss: 0.0723 - val_acc: 0.9838\n",
      "Epoch 4/5\n",
      "5668/5668 [==============================] - 3s 556us/step - loss: 0.0340 - acc: 0.9929 - val_loss: 0.0553 - val_acc: 0.9873\n",
      "Epoch 5/5\n",
      "5668/5668 [==============================] - 3s 554us/step - loss: 0.0164 - acc: 0.9970 - val_loss: 0.0574 - val_acc: 0.9887\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1aae38557b8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 120\n",
    "num_epoch = 5\n",
    "\n",
    "model.fit(train_X, train_y, batch_size=batch_size, epochs=num_epoch, validation_data=(test_X, test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## toy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text2vec(input_text):\n",
    "    input_text = input_text.lower()\n",
    "    input_text = re.sub(r1, '', input_text)\n",
    "    input_text = re.split(r2, input_text)\n",
    "    text_vec = []\n",
    "    \n",
    "    for word in input_text:\n",
    "        if word in word2ID.token2id.keys():\n",
    "            text_vec.append(word2ID.token2id[word])\n",
    "        else:\n",
    "            text_vec.append(num_word+1)     # 不存在于字典的词用总词数+1填充\n",
    "            \n",
    "    bu0 = max_lenth - len(text_vec)\n",
    "    text_vec.extend([num_word+2] * bu0)\n",
    "    return text_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\study\\Anaconda\\lib\\re.py:212: FutureWarning: split() requires a non-empty pattern match.\n",
      "  return _compile(pattern, flags).split(string, maxsplit)\n"
     ]
    }
   ],
   "source": [
    "input_text = 'You are so beautiful.'\n",
    "text_vec = np.array(text2vec(input_text))\n",
    "text_vec = text_vec.reshape(-1, text_vec.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_classes(text_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'You are so beautiful.'被正确预测为类别1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
